# 博客方案：AI Code Review 竞技场

## 定位

**中文标题：** "我让 3 个最新 AI 互审 Milvus 代码并互相辩论，然后让它们互相匿名打分"
**英文标题：** "AI Code Review Arena: Claude vs GPT vs Gemini Battle Over Real Milvus PRs"
**发布渠道：** 公众号 / 知乎（中文），Medium / 技术博客（英文）
**一句话概括：** 用真实开源项目 PR 做擂台，通过多轮对抗辩论 + 匿名互评打分，测试最新 AI 模型的代码审查能力。

---

## 评测设计

### 评测集：Milvus PR 选取

选 8 个 Milvus PR，分为两类：

| 类型 | 数量 | 选取标准 | 用途 |
|------|------|---------|------|
| 已知有 bug 的 PR | 8 个 | 后来被 revert 或有后续 fix 的 PR | 硬分（bug 检出率） |
| 正常优质 PR + 上述有bug的PR| 2 个 正常PR + 上述8个 | 中等复杂度、已 merge 的 PR | 软分（review 质量） |

**如何找已知有 bug 的 PR：**
- 在 Milvus repo 搜 revert commit 或 "fix regression" 类的 PR
- 倒推找到引入 bug 的原始 PR
- 该原始 PR 就是"考题"，已知 bug 就是"标准答案"

### Bug 难度分级（同权）

| 级别 | 定义 | 举例 |
|------|------|------|
| L1 局部 | 只看 diff 就能发现 | 逻辑错误、off-by-one、变量用错 |
| L2 上下文 | 需要理解周边代码 | 接口不匹配、资源未释放、错误处理缺失 |
| L3 深度 | 需要系统级理解 | 并发问题、边界条件、跨模块影响 |

### 执行方式：Magpie 辩论模式

每个 PR 跑 Magpie 完整辩论流程：
1. 上下文收集 + 预分析
2. Round 1：独立评审（所有模型并行 review，互不可见）
3. Round 2+：交叉辩论（模型互相质疑挑战）
4. 收敛判断 + 最终总结
5. Issue 去重 + 结构化输出

辩论记录一举两得：既是评测数据，也是博客内容素材。

### 让辩论更激烈：Prompt 改动

所有模型使用**完全相同的 Prompt**（公平公正），但在 Magpie 的辩论机制中强化对抗性：

**Round 2+ 辩论指令（改前）：**
- Challenge weak arguments - don't agree just to be polite

**Round 2+ 辩论指令（改后）：**
- 不要为了结束辩论而同意。如果你认为自己的观点是对的，用代码中的具体证据来捍卫它。
- 指出其他 reviewer 论点中的漏洞、缺失的上下文或错误的假设。
- 改变立场可以，但仅限于对方给出了你无法反驳的、基于代码的具体论据——而不是因为对方说得自信。
- 如果你在审查对方论据的过程中发现了新问题，也要提出来。

### 评分体系：双轨独立

#### 硬分：Bug 检出率

- Magpie使用`-r 1`参数，保证每个模型独立运行。
- 每个已知 bug PR：模型在 review 过程中是否发现了已知 bug？
- 发现 = 1 分，未发现 = 0 分，所有 bug 同权
- 误报数量单独统计（不扣分，独立展示）
- 按 L1/L2/L3 难度分组展示为柱状图

#### 软分：互评 Review 质量

- Magpie使用多轮辩论模式，保证每个模型都能看到其他模型的review。
- 辩论结束后，每个参赛模型开**新 session** 担任裁判
- Review 结果匿名化（Reviewer A / B / C）
- 每个裁判给所有 review（包括自己的，但匿名）打分，4 个维度：
  - 问题识别准确性（1-10）
  - 建议可操作性（1-10）
  - 分析深度（1-10）
  - 表达清晰度（1-10）
- 所有分数直接使用，不去高低分
- 裁判偏见模式本身也是有趣的内容

---

## 博客结构

### Section 1：引子（Hook）

新模型扎堆发布，benchmark 满天飞。但谁能在真实代码的战场上赢？我们用 Milvus 向量数据库的真实 PR 做擂台，让最新的 AI 模型互相 review、互相辩论，最后让它们匿名互评打分。结果如何？

### Section 2：实验设计

- 介绍 Magpie（多 AI 对抗式代码审查工具）
- 说明评测集（Milvus PR，选取标准）
- 说明评分体系（硬分 + 软分，两个独立维度）
- 关键细节：模型不知道自己在被评测

### Section 3：擂台实录（精选 2-3 个 PR 详细展示）

从 5-8 个评测 PR 中，挑 2-3 个最有看点的做详细展示。

**每个 PR 的呈现结构：**
```
PR 简介（3-5 句话，不熟悉 Milvus 的读者也能看懂）
  ↓
Round 1：独立评审（并排对比 3 个模型各自的发现）
  ↓
辩论精华（最激烈的交锋片段）
  ↓
揭晓：这个 PR 真实存在的问题是什么，谁找到了
```

**精选 PR 的标准：**
- PR1：展示"谁找到了隐藏 bug"（硬分的戏剧性）
- PR2：展示"辩论中的反转"（模型改变立场）
- PR3：展示"分析深度的差异"（质量差距）

**最出彩的素材类型（优先找这些）：**
- 模型 A 找到了真实 bug，但模型 B 在辩论中成功反驳了它（误杀正确意见）
- 某个模型提出了开发者本人都没想到的 insight
- 模型在辩论中改变立场（从"没问题"变成"确实有问题"）

**写作技巧：**
- PR 描述用通用语言（"这个 PR 给分布式索引模块加了一个缓存层"），让不熟悉 Milvus 的读者也能跟上
- 选最有戏剧性的辩论交锋
- 每个精选 PR 突出不同的看点

### Section 4：匿名互评打分

- 用表格展示评分结果（裁判 × 维度 × 选手）
- "揭晓时刻"——把 Reviewer A/B/C 对应回模型名字
- 如果裁判之间分歧大，这本身就是好素材（"连 AI 裁判都吵起来了"）
- 重点分析裁判偏见模式（"有没有模型给自己打高分？"）

### Section 5：排行榜 + 分析

**两张独立排行榜：**
- 硬分榜：Bug 检出率（柱状图，按 L1/L2/L3 分组）
- 软分榜：Review 质量（雷达图，4 个维度）

**分析角度（选 2-3 个写）：**
- 哪个模型最稳（方差最小）vs 哪个最极端
- L1→L3 难度升级时，各模型的表现衰减曲线
- 辩论中谁更坚持己见 vs 谁容易妥协
- 误报率对比
- 裁判偏见分析（每个模型给别人 vs 给自己的打分差异）

### Section 6：结尾

- "本次评测使用的工具是 Magpie，一个开源的多 AI 对抗式代码审查工具"（附链接）
- "评测素材来自 Milvus 向量数据库的真实 PR"（附链接）
- 不硬推——读者看完全文自然会对工具和项目产生兴趣

---

## 风险预案

| 风险 | 应对 |
|------|------|
| 没有区分度（所有模型表现差不多） | "最新模型在代码审查上趋于同质化"本身就是有新闻价值的结论 |
| 已知 bug 太简单 / 太难 | 选 PR 时确保 L1-L3 各级别都有覆盖 |
| 多次运行结果不一致 | 可以跑 2-3 次；博客中说明这一点 |
| 辩论不够精彩 | 选有真正复杂度的 PR；强化后的辩论 Prompt 会增加对抗性 |

---

## 需要的视觉素材

1. 实验设计流程图
2. Bug 检出率柱状图（按 L1/L2/L3 分组）
3. Review 质量雷达图（每个模型 4 个维度）
4. 辩论记录截图（做成对话气泡格式更好看）
5. 最终排行榜图表
6. 社交媒体预热图（排行榜 + 一句有争议的结论）

---

## 社交媒体传播

- 预热：一张排行榜图 + 一句挑衅性的结论（如"最贵的模型不是最强的 reviewer"）
- 连载格式：每条推文/帖子分享一个辩论精彩片段
- 互动钩子："你能猜出这段 review 是哪个 AI 写的吗？"配匿名 review 截图

---

## 执行清单

- [ ] 找 5-8 个合适的 Milvus PR（3-4 个已知有 bug，2-3 个正常 PR）
- [ ] 给每个已知 bug 标注 L1/L2/L3 难度
- [ ] 将新模型接入 Magpie（如果尚未支持）
- [ ] 修改 Magpie 辩论 Prompt（强化对抗性）
- [ ] 对每个 PR 跑 Magpie 辩论模式
- [ ] 跑软分评测（每个模型做匿名裁判）
- [ ] 精选 2-3 个最有看点的 PR 做详细展示
- [ ] 制作可视化图表
- [ ] 撰写中文版（公众号/知乎）
- [ ] 撰写英文版（Medium/技术博客）
- [ ] 准备社交媒体预热内容
- [ ] 发布 & 推广
